效率真心高呀，神航内推完HR小姐姐就来了。

## 9.6-HR约笔试 | 9.7-笔试

询问HR得知不能公开笔试形式和内容。全是算法，挺有意思的。

## 9.22-一面

<!--~~ddb强度挺大呀，HR催了三次面试官大哥才看的我的笔试代码。~~-->

一个小时System拷打酣畅淋漓，面试官是秋招第二个按着简历一个一个问的，呜呜好感动。

+ 面试官设备出问题不能开摄像头，所以也没限制我是否开摄像头。
+ 询问是否可以录屏，同意并叮嘱不能传播，于是进一步问是否可以写面经，答复可以。
+ 自我介绍。

1. 你是怎么设计学习计划的，已经学了哪些，还计划学什么？
2. 解释器：
	+ 是课程作业还是什么课程？
3. Redis的Modern C++实现
	+ 哪里体现Modern？
		+ RAII管理资源，比如文件描述符和套接字。
		+ 标准库，`Optional`，并发相关。
		+ Redis的有序集使用侵入式数据结构实现，如果要Modern的话只能CTRP，这里本身就不好实现，而且对析构需要异步，所以目前只能说有方向，但是没有实现。
		+ 项目结构借鉴了CS144和CMU15445的CMakeLists.txt
	+ 你是具体是怎么实现的，比如网络模块和存储模块的实现，和真实的Redis有哪些异同。
		+ 网络通信：
			+ 对C风格系统调用接口的封装，不仅是调用的封装，还有其安全的管理，比如对文件描述符的获取和释放，套接字的抽象，对每个TCP链接的抽象，对于`poll`的异步IO的字节缓冲流的抽象封装。
		+ 存储部分：主要是各种数据结构的实现
			+ 有序集是能渐进调整的，基于拉链法的哈希表和二叉平衡树AVL树的使用侵入式数据结构技术的结合。
				+ Redis使用跳表。
			+ 有序链表实现定时器。
			+ 堆管理键的TLS
				+ 惰性删除和RAII的冲突（上面提到）
	+ 是单线程模型嘛？
		+ 在确定删除和进行删除之间有一个小异步。
4. 说说你对CS144的实现，从send，到另一端的receive，你的实现经过哪些网络层次，涉及哪些算法，或者说就是讲讲你的实现。
	+ CS144提供的基础设施：
		+ 之前讲的对C风格接口或者系统调用的封装。
		+ 数据类
	+ 字节流：二倍大小循环队列实现字节流缓冲。
	+ 重排器：使用链表维护不重叠区间。
	+ TCP的收发两端就是按照接口普通的模拟
	+ 我做的2023没有对TCP状态机的实现
	+ 其他就是零碎的比如将TCP数据流到IP数据包的包装和拆包，从IP地址到MAC的相互转换。

	>这里我还忘了，共享屏幕看的笔记才想起来的。

	性能测试使用的是官方提供还是自己的方法进行测试？
 
5. 讲一讲你量化交易系统中的优化部分。
	+ 调整系统调用代码的分布
	+ 对不同的字符串到枚举的缓冲
	+ 去掉不必要的安全性检测。
 
6. xv6中也提到对系统调用的优化，都指的哪里？
	+ 共享页
	+ COW

	+ 时间相关系统调用：
		+ 你在Redis中应该也有时间相关的需求，包括精度之类的，你是怎么做的？
			仅仅是获取当前时间，精度是毫秒，从C++标准库拿到。

		+ 你对操作系统关于获取时间的相关服务和接口有哪些了解？

		如果要测试性能，获取时间本身操作的开销就有影响，而且跟时间相关的系统调用，Linux有很多对其的优化，和你在简历上说的很类似。

+ 复盘：
	+ Reference：[How Time Work](https://hoswey.github.io/2020/08/25/how-time-work/)
	+ 系统调用`gettimeofday`
		```c
		#include <sys/time.h>
		int gettimeofday(struct timeval *tv, struct timezone *tz);
		```
		现代操作系统主板上会有一个Real Time Clock，记录当前时间，通过主板电池CMOS Battery维持，电池没电则出问题  
		机器启动会读取这里的值保存到Kernel中，然后再有一个寄存器TSC，保存的是CPU的cycle（频率的倒数）个数，然后计算当前时间

		这里肯定会进入内核态的。

	+ `gettimeofday`调用系统调用`clock_gettime`

7. 你在分析量化交易系统的性能瓶颈时使用火焰图，但是并没有找到性能瓶颈，你觉得这是为什么？  
	你当时的火焰图是用什么生成的？  
	它采集和profile的原理是什么？不是获取运行时栈的信息，是采样时怎么知道每个段的耗时，这个在C++中是怎么样的？

+ 首先强调两个性质和三个概念：
	+ 有序性：对一个变量的读写是原子的
	+ 可见性：对一个变量的写完成后立刻被其他线程可见

	为什么会出现上面的问题？我目前的理解是现代体系结构中，内存中的信息是读取到寄存器中，然后进行计算，即分成读、算、写三步，如果出现这三步没有完成即出现了上下文切换则出现错误。同时现在CPU通常有缓存，即一个线程完成对内存的写后，未必立刻就写回到对应的内存中，此时其他指令对该位置的读则出现错误。
	>还有其他各种乱序执行、流水线、分支预测、多级缓存。

	+ 内存屏障Memory Barrier：保证以上两个性质的实现的软硬件指令，是体系结构层次
	+ 内存模型Memory Model：即编程语言为实现以上两个性质的对内存屏障的语义定义和使用规则
	+ 内存序Memory Order：我理解是内存模型语境下有序性的名称，通常有顺序一致性（Sequential Consistency）、释放-获取序（Release-Acquire Ordering）、强制排序（Total Store Ordering），强度一次递减。
		>但是我并不能直觉的区分这三者区别
 
+ Q1：
	```cpp
	struct SpinLock {
		void lock() {
			while (locked) {
				// do nothing
			}
			locked = true;
		}
		void unlock() {
			locked = false;
		}
		bool locked = false;
	};
	```

	上面自旋锁的实现有什么问题？

	+ 首先是竟态问题，这里`locked`变量的读写可能会出现竟态。

	+ 我在回答的时候还答错一个，即这里自旋锁是空转的，白白浪费了CPU。实际上，这就是自旋锁的性质，它就是空转CPU的，它带来的问题应该通过程序员的最佳实践解决。

	比如这样实现：
	```cpp
	struct SpinLock {
	    std::atomic_flag locked = ATOMIC_FLAG_INIT;
		void lock() {
			while (locked.test_and_set(std::memory_order_acquire)) {}
		}
		void unlock() {
			locked.clear(std::memory_order_release);
		}
	};
	```

	或者hiki佬的实现
	```cpp
	class SpinLock {
		std::atomic<bool> flag{false};
	public:
		void lock() {
			while (flag.exchange(true, std::memory_order_acquire));
		}
		void unlock() {
			flag.store(false, std::memory_order_release);
		}
	};
	```

	所以用什么内存序？

	即上面的代码实现

	+ 这里即使用释放-获取序：
		+ acquire保证在该原子操作完成之前，所以对共享数据的加载都不会被重排到该原子操作之后
		+ release保证在该原子操作完成之后，所有对共享数据的加载都不会被重拍到该原子操作之前

+ Q2：
	```cpp
	struct MultiLockGuard {
		MultiLockGuard(std::initialize_list<SpinLock*> ls) : locks(ls) {
			for (auto l: locks) l->lock;
		}
		~MultiLockGuard() {
			for (auto l: locks) {
				l->unlock();
			}
		}
		std::vector<SpinLock*> locks;
	};
	```

	上面代码有什么问题？

	+ 死锁：
		+ 一方面不能保证用户传入的锁列表是正确的，比如把一个锁两次放进list中，那就死锁了，甚至用户可以直接传入空指针，所以应该要做检测和去重。
		+ 另外就是放的顺序可能导致死锁，比如一个的锁列表是`a, b`，另一个是`b, a`，两个线程同时拿到了自己列表的第一个锁，然后就死锁了。
		+ 还有在hiki佬指出的拿放锁的最佳实践问题：解锁顺序一般与枷锁顺序相反。因为一般来说越后面lock的锁，它的scope应该越小，也应该越早释放，而且正序解锁还有一个线程频繁切换的问题。即如果有两个多个锁的锁列表是一样的，如果正序释放，一个线程释放一个锁，另一个线程就抢掉了这个锁，而之后的锁不能立刻抢到，还得等释放的线程接着释放。

+ Q3：
	```cpp
	struct ThreadPool {
		ThreadPool(std::size_t num) {
			for (decltype(num) i = 0; i < num; ++ i) {
				workers.push_back(std::thread(poll, this));
			}
		}
		~ThreadPool() {
			stop.store(true, std::memory_order_seq_cst);
		}
		void push(std::packaged_task<int()>&& task) {
			taskqueue.push(std::move(task));
		}
		static void poll(ThreadPool* pool) {
			while (!poll->stop.load(std::memory_order_seq_cst)) {
				auto task = std::move(pool->taskqueue.front());
				pool->taskqueue.pop();
				task();
			}
		}
		std::atomic<bool> stop;
		std::vector<std::thread> workers;
		std::queue<std::package_task<int()>> taskqueue;
	};
	```

	+ 这里的
		```cpp
		stop.store(true, std::memory_order_seq_cst)
		stop.load(std::memory_order_seq_cst)
		```

		即顺序一致的内存序

	上面代码有什么正确性问题和性能问题
	>这里对push和poll两个操作没有加锁，先忽略。
	
	+ 这个`push_back(this)`这个事儿就得掂量掂量，比如用户使用智能指针维护线程池，而这个poll是静态的，很有可能poll拿到的ThreadPool的指针已经被其他智能指针释放了。有标准库解决这个问题，即公有继承` std::enable_shared_from_this<T>`，这也是一个CRTP，这个类有成员函数`shared_from_this()`解决这个问题。
	+ 性能问题：初始化那里，通过`resize`初始化。

	你写多线程写的多吗？比如有个接口叫做`join`知道么？它是否可以解决这个线程池的一些问题

	+ 在这个线程池的析构里，仅仅通过一个原子变量去表示该线程池是否有效，在此之后确实不会再有新的线程开始，但是之前的线程可能仍然在跑，这时它们的语义就是奇怪的，很反直觉。所以这里要`join`下，即在线程池析构前，保证为之前维护的线程都已经运行完毕。

	好，你上面提到传递`this`的问题，即使使用智能指针维护，但是这里把`this`指针给到`poll`，然后这个`poll`本身还在线程池中，那么这里可不可能出现循环引用的问题呢？

	确实会的，如果使用智能指针维护了，比如外界使用智能指针维护这个线程池，然后线程池中，`std::vector`的每个成员都维护一个`poll`的线程，而线程中每个实例也都拿着这个线程池的智能指针。那么这会发生什么呢？即只有当线程池析构的时候，才会析构这里的`std::vector`，它们里面的线程池本身的智能指针才会减少对应的引用计数。但是在析构前引用计数永远不会减少到0，永远不会析构。循环引用了。

	+ 对这里的队列的使用没有加锁。

	还有没有性能问题，比如如果构造函数参数`num`很大，每个线程的`poll`运行都会有一个锁的竞争，这里竞争可能很大，怎么办？你在做xv6时有没有相关问题，比如如何降低对锁的竞争？我印象里有就是将其拆分成多个锁，这样每个锁的竞争更少。
	>这里我依然不能直观的理解。

	在Usamoi佬的提示下，可以使用**work-steal**解决：保留线程池的FIFO队列，为每个worker都提供一个双端队列，所有对线程池的Push都是放进FIFO队列，然后有单独的线程调度这个FIFO讲其中的任务给到各个worker的队列中，每个worker只从自己的队列拿任务，这样就降低了锁竞争，本质还是锁拆分。

	![](https://cdn.jsdelivr.net/gh/zweix123/CS-notes@master/resource/recruit/usamoi-in-work-steal.png)

+  Q4：
	```cpp
	int doFib(ThreadPool& p, int n) {
		if (n < 2) {
			return n;
		} else {
			std::packaged_task<int()> taskx([&]{doFib(p, n - 1); }); // wrap the function
			std::future<int> fx = taskx.get_future();  // get a future
			p.push(std::move(taskx));
			
			std::packaged_task<int()> tasky([&]{doFib(p, n - 2); }); // wrap the function
			std::future<int> fy = taskx.get_future();  // get a future
			p.push(std::move(tasky));

			fx.wait();
			fy.wait();

			int x, y;
			x = fx.get();
			y = fy.get();
			return x + y;

		}
	}

	int fib(int n) {
		ThreadPool p(100);
		return doFib(p, n);
	}
	```

	上面哪里会死锁？

	这里的`wait`和`get`会等待子线程，但是如果线程池容量比较小，大量的线程的都会等待它的子结点的计算，但是线程池已经占满了，后面的几点不会被计算，死锁。

	怎么解决？

	给每个线程提供主动放权的操作。
	>我以为只能在C++标准库上做文章，但是面试官的意思是需要编译器或者操作系统提供支持。
 
	相当于要提供一个接口类似有栈协程的主动放权。

	那我看你在xv6里也有一个用户级线程的实现
	+ 主要有两个：
		+ 一个是提供系统调用允许用户设置几个CPU后周期对某个线程进行中断
		+ 另一个是用户级线程切换，即编写汇编代码，在不进入内核态的情况下保存和切换寄存器上下文。

+ 反问：
	+ 面评

>自我感觉回答的还是比较差劲的，感谢面试官给了通过。


## 10.10-二面 | 10.12-挂

不允许录屏、不允许传播面经。

实际上只有一个内容，开放寻址法一次探测哈希表，估计后面还想进一步问来着。但是这里确实懵住了，面试时间短就只问了这一个，最后反问的时候也心灰意冷没有争取下了。

粘一个我在面试后的实现吧：https://github.com/zweix123/ACT/blob/main/include/stl/hashtable.h

太可惜了，DDB算是Dream Work之一吧，半场开香槟也太SB了。
