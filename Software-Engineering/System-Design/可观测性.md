+ Ref：
    + [凤凰架构 · 可观测性](https://icyfenix.cn/distribution/observability/)
    + [Shalvah Adebayo · Observability Basics](https://drive.google.com/file/d/1-NjkbAXrbjaNLZU6UygzNIrUmZgQ5K_F/view?pli=1)

## 概述

+ 名词的由来：[可观测性（Observability）](https://en.wikipedia.org/wiki/Observability)原本是与[可控制性（Controllability）](https://en.wikipedia.org/wiki/Controllability)一起是由匈牙利数学家Rudolf E. Kálmán针对线性动态控制系统提出的一组对偶属性，表示“可以由其外部输出推断其内部状态的程度”。

所以这个名词是从控制论的舶来品。

学术界通过将其分成三个各有侧重但并不完全独立的概念：Peter Bourgon在"2017 Distributed Tracing Summit"之后，撰写总结文章[《Metrics, tracing, and logging》](https://peter.bourgon.org/blog/2017/02/21/metrics-tracing-and-logging.html)系统地阐述了这三者的定义、特征，以及它们之间的关系与差异，受到了业界的广泛认可。

![三个方向](https://cdn.jsdelivr.net/gh/zweix123/CS-notes@master/resource/blog/三个方向.png)

+ 聚合度量（Metrics）：主要特征是可聚合的（aggregatable），即 将数据聚合作为一个图表的原子数据。目的是监控（Monitoring）和预警（Alert）。
+ 链路追踪（Tracing）（国内还称为全链路追踪或者分布式追踪）：请求范围（request-scoped）内的信息，一个事务的生命周期内的元数据。直觉的说，调用链，可以类比单机程序中的调用栈。目的是排查故障和分析性能，比如分析调用链的那一部分、哪个方法出现错误或阻塞，输入输出是否符合预期等等。
+ 事件日志（Logging）：处理“分布的”事件（discrete events（这里的discrete与其翻译为离散不如理解为分布式中的分布的））

还有一个视角是关于每个方向的成本：Metrics时最低的，因为它有压缩的属性，而Logging则毫无疑问是最高的。

下面会讲述下每个方向，但是值得说的，每个方向并非完全独立。部分工具除了以某个方向为核心外，在其他方向也有效果。

## Logging

+ 产品：Elastic Stack（ELK）技术栈一家独大
    + Logstash可能被Fluentd取代，变成EFK技术栈

+ 步骤链条：
    1. 应用输出日志
    2. 收集：ELK的Logstash
    3. 缓冲
    4. 聚合、加工：ELK的Logstash
    5. 索引、存储
    6. 分析、查询


+ 结构化：不一定是text，可以是机器可读的

### 1.输出

+ 最佳实践
    + 格式统一
    + 内容“不多”
        + 避免打印敏感信息
        + 避免引用慢操作：包含通过数据库读、网络通信或者打印较大数据结构
        + 避免打印追踪诊断信息：这部分应该属于Tracing的内容。
        + 避免误导他人。
    + 内容“不少”
        + 处理请求时的TraceID：怪，这不是Tracing的概念么？
        + 系统运行过程中的关键事件
            + 操作
            + 与预期不符
            + 未能处理的异常和警告
            + 自动任务
        + 启动时输出配置信息

### 2.收集与缓冲
写日志是在服务节点中，不能在节点中分别建设查询能力，不仅是资源和工作量的问题，因为一个请求通常跨过多个节点，必须将日志统一收集。由此催生专门的日志收集器。

Logstash在各服务节点部署收集的客户端Shipper，同时单独部署作为收集日志的服务端Master。

Logstash及其插件由JRuby编写，需要跑在单独的Java虚拟进程中，对服务的负担较重。

Elastic.co公司将客户端整理成以[libbeat](https://github.com/elastic/beats/tree/master/libbeat)为核心的[beats](https://github.com/elastic/beats)框架，并重写为[filebeat](https://github.com/elastic/beats/tree/master/filebeat)。

这里的beats框架是一个规模相当大的家族，一定程度也有Tracing和Metrics的能力。

+ 连续性：日志的量是相当大的，收集到系统中的日志与实际产生的日志保持绝对的一致非常困难，也不应该付出过高的成本。换言之，日志不追求绝对的完整精确，只追求在可承受范围内尽可能高的保证质量。
    + 常见的做法：将日志接收者从Logstash或者Elasticsearch转移至抗压能力更强的队列缓存，比如Kafka或者Redis，此两者也通常放在Logstash或者Elasticsearch之前。

### 3.加工与聚合

日志在收集之后，放在Elasticsearch之前，还要进行加工转换与聚合处理。因为日志是非结构化数据。如果不做处理Elasticsearch只能用全文检索的原始方式使用日志。

Logstash能通过Grok表达式语法转换成结构化日志，以Json（或者其他）给到Elasticsearch。

>Logstash还有一定的聚合能力。

### 4.存储和查询

+ 一家独大（毫无疑问版）：Elasticsearch
    + 原因：
        + 日志呈现流数据特性：
            + 写入数据不变
            + 最近数据就是热数据，久的数据就是冷数据，方便冷热分离
        + 实时性强
    + 通常搭配Kibana作为前端

## Tracing

+ 产品：
    + 因为服务之间的通信方式、服务的实现语言，直接影响Tracing的方式，而Tracing通常更具有侵入性，所有没有统一所有场景的工具。
    + 现代分布式链路追踪公认的起源是Google的Dapper
        + 2004年内部开始使用，2010年发布论文[《Dapper, a Large-Scale Distributed Systems Tracing Infrastructure》](https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/papers/dapper-2010-1.pdf)
        + 很多产品收其直接影响
            + Naver（Line母公司）的[pinpoint](https://github.com/pinpoint-apm/pinpoint)：其实比Dapper更早，因为Dapper的论文有引用。
            + Twitter的[zipkin](https://github.com/openzipkin/zipkin)
            + 阿里的鹰眼
            + 大众点评的[cat](https://github.com/dianping/cat)
            + Apache的[skywalking](https://github.com/apache/skywalking)

+ 分类：
    + 数据收集
    + 数据存储
    + 数据展示

    广义的Tracing也叫APM（Application Performance Management）；狭义的Tracing则只包含数据收集。比如[spring-cloud-sleuth](https://spring.io/projects/spring-cloud-sleuth)，搭配Elasticsearch作为数据存储，搭配Zipkin作为数据展示。

+ Dapper提出两个概念
    + 跨度Span：
        + 足够简单，以便放在日志或者网络通信的头中
        + 包含基本信息：时间戳、起止时间、Trace的ID，Sapce的ID，父Span的ID
    + 追踪Trace：一次Trace就是有顺序、有层级关系的Span的Trace Tree

+ 挑战：
    + 功能性：服务异构性，不同语言、不同网络协议
    + 非功能性：
        + 低性能损耗
        + 对应用透明
        + 随应用扩缩
        + 持续的监控

### 关于数据收集

有三种方法
+ 基于日志（Log-Based Tracing）：
    + 优点：
        + 对网络消息没有侵入性
        + 对代码侵入性比较低，对性能影响低
    + 缺点：
        + 依赖日志收集，而日志不追求绝对的连续性和一致性。
+ 基于服务（Service-Based Tracing）：主流，通过给目标应用注入追踪探针Probe，性能负担大。
+ 基于边车代理（Sidecar-Based Tracing）：服务网络的专属方案，对应用完全透明，缺点是只能实现服务之间的调用层面
    + 产品：[envoy](https://www.envoyproxy.io/)（但没有UI，可以使用其他产品作为UI）

### 关于规范化问题

由于没有统治级产品，且Dapper论文只提供思路而没有规范标准，所以最初比较混乱。

+ 2016.11，CNCF接收OpenTracing作为基金会的第三个项目，是一套协议规范。
    + 软件和Tracing系统之间
    + 微服务之间

+ Google提出OpenCensus规范并得到Microsoft支持，该规范范围更广。

+ 2019，上面两服务握手言和，发布OpenTelemetry，其不见包含Tracing规范，还包含Logging和Metrics的规范，还有其他等等。

## Metrics

+ 产品：
    + 云原生：Prometheus普罗米修斯（CNCF的第二个项目）（随着Kubernetes统一容器编排）

+ 部分：
    + 指标收集
    + 存储查询
    + 监控预警

### 1.指标收集

+ 两个核心问题
    + 如何定义Metrics
    + 如何将Metrics告知服务器

+ 指标类型（Metrics Type）
    + 计数度量器（Counter）：比如调用量
    + 瞬态度量器（Gauge）：机器指标
    + 吞吐率度量器（Meter）：
    + 直方图度量器（Histogram）：
    + 采样点分位图度量器（Quantile Summary）：

+ 拉取式采集（Pull-Based Metrics Collection）：Prometheus（[原因](https://prometheus.io/docs/introduction/faq/#why-do-you-pull-rather-than-push?)）、Datadog、Collectd
+ 推送式采集（Push-Based Metrics Collection）：Ganglia、Graphite、StatsD

Exporter 是 Prometheus 提出的概念，它是目标应用的代表，既可以独立运行，也可以与应用运行在同一个进程中，只要集成 Prometheus 的 Client Library 便可。Exporter 以 HTTP 协议（Prometheus 在 2.0 版本之前支持过 Protocol Buffer，目前已不再支持）返回符合 Prometheus 格式要求的文本数据给 Prometheus 服务器。

### 2.存储查询-时序数据库

写操作，时序数据通常只是追加，很少删改或者根本不允许删改。针对数据热点只集中在近期数据、多写少读、几乎不删改、数据只顺序追加这些特点，时序数据库被允许做出很激进的存储、访问和保留策略（Retention Policies）：

以日志结构的合并树（Log Structured Merge Tree，LSM-Tree）代替传统关系型数据库中的B+Tree作为存储结构，LSM 适合的应用场景就是写多读少，且几乎不删改的数据。
设置激进的数据保留策略，譬如根据过期时间（TTL）自动删除相关数据以节省存储空间，同时提高查询性能。对于普通数据库来说，数据会存储一段时间后就会被自动删除这种事情是不可想象的。
对数据进行再采样（Resampling）以节省空间，譬如最近几天的数据可能需要精确到秒，而查询一个月前的数据时，只需要精确到天，查询一年前的数据时，只要精确到周就够了，这样将数据重新采样汇总就可以极大节省存储空间。
时序数据库中甚至还有一种并不罕见却更加极端的形式，叫作轮替型数据库（Round Robin Database，RRD），以环形缓冲（在“服务端缓存”一节介绍过）的思路实现，只能存储固定数量的最新数据，超期或超过容量的数据就会被轮替覆盖，因此也有着固定的数据库容量，却能接受无限量的数据输入。

### 3.监控预警

在生产环境下，大多是 Prometheus 配合 Grafana 来进行展示的，这是 Prometheus 官方推荐的组合方案，但该组合也并非唯一选择，如果要搭配 Kibana 甚至 SkyWalking（8.x 版之后的 SkyWalking 支持从 Prometheus 获取度量数据）来使用也都是完全可行的。

## 其他

+ 美团：B站
+ 滴滴：B站
+ B站：https://mp.weixin.qq.com/s/gTB_hEXJQ2gz_oP7VN3-dg

https://mp.weixin.qq.com/s/M6o74ME181iBZZkM42hmXw